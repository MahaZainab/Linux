# Auburn University Remote Cluster Tutorial

This guide walks you through the process of connecting to Auburn University‚Äôs(AIAU) cluster.  
The cluster provides **powerful GPUs, CPUs, and large memory** resources to run heavy computations, simulations, and deep learning tasks.

---

## 1. Request Access
Before you begin, you must have an **Auburn AIAU account**.

- Contact your **system administrator** or **HPC support team**.  
- Request access and provide your Auburn username.  
- Credentials (username/password) will be issued.  
- Some systems also require **Duo two-factor authentication**.

---

## üíª 2. Connect to the Cluster (SSH)
You will connect to the cluster using **SSH**.

- On **Windows**:  
  Press `Win + R`, type `cmd`, and press Enter.  

- On **Mac/Linux**:  
  Open the **Terminal**.

Run the following command (replace with your actual username):

```bash
ssh your_username@easley.hpc.auburn.edu

```
## üìÇ 3. Transferring Files

To run scripts or access results, transfer files using scp:

### Upload a file to the cluster:
```bash
scp myscript.py your_username@easley.hpc.auburn.edu:/home/your_username/
```

### Download a file from the cluster:
```bash
scp your_username@easley.hpc.auburn.edu:/home/your_username/results.txt .
```

For larger datasets, use **rsync** for faster transfers.

---

## üñ•Ô∏è 4. Working on the Cluster

The HPC system runs on Linux. Some useful commands:

```bash
ls        # list files
cd dir    # change directory
pwd       # print working directory
```

## üì¶ Modules
```bash
module avail          # list available modules
module load <module>  # load a module
module list           # show loaded modules
module purge          # unload all modules
```

---

## üñ•Ô∏è Job Allocation (Slurm)
```bash
salloc -N1                          # request 1 node
salloc -N1 --nodelist=aiau010        # request a specific node
srun --pty /bin/bash                 # run shell on allocated node
```

---

## üìú Batch Jobs
```bash
sbatch job.slurm     # submit job
squeue -u username   # check job status
scancel job_id       # cancel a job
```

---

## üìÇ File Transfer
```bash
scp file.py user@aiau.eng.auburn.edu:/home/user/   # upload
scp user@aiau.eng.auburn.edu:/home/user/out.txt .  # download
rsync -avP data/ user@aiau.eng.auburn.edu:/home/user/data/   # sync large datasets
```

---

## üìä GPU Monitoring
```bash
nvidia-smi
```

---

## ‚ö° Python Test (CUDA)
```python
import torch
print(torch.tensor([1,2]).cuda())
```

---

## ‚úÖ Quick Summary
- **Login:** `ssh user@aiau.eng.auburn.edu`
- **Modules:** `module avail`, `module load <pkg>`
- **Jobs:** `salloc`, `sbatch`, `squeue`
- **Cancel:** `scancel job_id`
- **Files:** `scp`, `rsync`
- **GPU:** `nvidia-smi`


### Requesting Interactive Resources

You must request resources through the job scheduler:

```bash
srun --partition=gpu --gres=gpu:1 --mem=16G --time=02:00:00 --pty bash
```

This gives you **1 GPU, 16 GB RAM, for 2 hours.**

---

## üìú 5. Submitting Batch Jobs

Most jobs are submitted with a batch script (`job.slurm`):

```bash
#!/bin/bash
#SBATCH --job-name=test_job
#SBATCH --output=output.txt
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --mem=16G
#SBATCH --time=04:00:00

module load python/3.9
python myscript.py
```

### Submit the job:
```bash
sbatch job.slurm
```

### Check its status:
```bash
squeue -u your_username
```

### Cancel a job:
```bash
scancel job_id
```

---

## ‚ö° 6. Best Practices

- Do not run heavy computations on the login node ‚Äî always use `srun` or `sbatch`.
- Request only the resources you need (GPUs, memory, time).
- Run large workloads from `/scratch` instead of `/home`.
- Use `module avail` and `module load <package>` to manage software environments.
- Use `screen` or `tmux` for long interactive sessions.
- Clean up files in `/scratch` regularly (purged after 30 days of inactivity).
